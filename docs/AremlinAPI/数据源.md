
> 在AbutionGraph中，  
Knowledges对象（包含Entity/Edge）是数据的存储和表达形式；  
KnowledgeId（包含EntityId（EntityKey）和EdgeId（EdgeKey））是Knowledges的原子对象；  
Vertex、Source和Target是KnowledgeId的原子对象。  
所有的操作都将围绕这些数据对象进行操作。

#### Abution数据源
##### AddKnowledge
AbutionGraph的数据接入功能，可单条或者批量添加。通用的数据源接口，可从任何数据源生成元素后添加到AbutionGraph。  
```java
// 定义的数据集合
List<Knowledge> knowledges = Arrays.asList(
        Knowledge.dimV("entity-type-1") //实体数据（有维度标签、节点和属性信息）
                .vertex("vertex-1")
                .property("property-1", "property-1-value")
                .build(),
        Knowledge.dimE("edge-type-2")  //关系数据（有维度标签、起止点和属性信息）
                .src("source-vertex-1")
                .dst("dest-vertex-1")
                .directed(true)
                .property("property-2", "property-2-value")
                .build()
);

g.addKnow(knowledges).validate(false).skipInvalid(false).exec(new User());
```


#### Flink数据源
Flink处于高速发展期，版本更新快，我们对Flink-1.7.0到Flink-1.12.1做了适应性测试，目前使用参考版本是Flink-1.9.x/1.12.1。  

Flink的扩展不包含在原始图功能中，使用前请先在AbutionProperties中启用扩展，如下所示：  
```java
        AbutionProperties properties = null;
        properties.enableFlinkSocket();
        properties.enableFlinkKafka();
        properties.enableFlinkRocketMQ();
        // .....
```
注意：Abution-Flink的写入不是一条一条的，AbutionGraph中具有缓冲，默认缓冲区大小的100w条图数据，超过此值将阻塞，以避免数据丢失和OOM，改变的参数是abution.flink.config.max-queue-size（FlinkConstants.MAX_QUEUE_SIZE）。  

##### AddKnowledgeFromFile
从文件中添加元素，Flink不是持久运行的，处理完文件后，进程也随之停止。  
```java
g.dataSource().flinkFile()
        .filename("path/filename")               // 文件路径
        .generator(TestGeneratorImpl.class)    // 数据处理为元素的逻辑程序
        .parallelism(2)                            // Flink进程的并行度
        .validate(true)                            // 验证元素是否正确-与图Schema是否对应
        .skipInvalidKnowledges(false)              // 是否跳过不合规的元素-该条数据可另作处理或丢弃
        .exec(user);
```

##### AddKnowledgeFromKafka
该操作针对实时性追求到毫秒级的数据入库任务，我们整合了较新版本的消息流缓冲组件Kafka-2.2.x。老版本的Kafka我们也整合在了AbutionGDB中，文档在此不列出。  
```java
g.dataSource().flinkKafka()
        .parallelism(1)                                            // Flink并行度
        .topic("test")                                             //  (必填)Kafka Topic
        .groupId("addKnowledgeDimension")                            //  (必填)Kafka 消费组
        .bootstrapServers("www.thutmose.cn:9092")         // (必填)连接Kafka的ip:port
        .generator(TestGeneratorImpl.class)                   // (必填)数据处理逻辑程序
        .option(FlinkConstants.MAX_QUEUE_SIZE, "1000000")   // Flink数据缓冲区允许的最大条数-数据量大时用于暂存数据后批量入库
        .option(FlinkConstants.SKIP_REBALANCING, "true")    // 是否跳过在每次插入数据后做集群数据均衡分布
        .exec(user);
```

##### AddKnowledgeFromRocketMQ
该操作针对实时性追求到毫秒级的数据入库任务，因为开发之时官方还没出Flink-MQ连接器，所以我们单独将其实现在依赖rocketmq-flink-addknowledges-1.0.2.jar中。  
```java
g.dataSource().flinkRocketMQ()
        .parallelism(1)                                            // Flink并行度
        .topic("test")                                             //  (必填)Kafka Topic
        .groupId("addKnowledgeDimension")                       //  (必填)Kafka 消费组
        .nameServerAddrs("master.thutmose.cn:9092")          // (必填)连接到MQ的ip:port
        .generator(TestGeneratorImpl.class)                   // (必填)数据处理逻辑程序
        .option(FlinkConstants.MAX_QUEUE_SIZE, "1000000")   // Flink数据缓冲区允许的最大条数-数据量大时用于暂存数据后批量入库
        .option(FlinkConstants.SKIP_REBALANCING, "true")    // 是否跳过在每次插入数据后做集群数据均衡分布
        .exec(user);
```

##### AddKnowledgeFromSocket
该操作一般应用于客户端请求添加数据。  

```java
g.dataSource().flinkSocket()
        .generator(TestGeneratorImpl.class)             // (必填)数据处理逻辑程序
        .parallelism(2)                                     // Flink并行度
        .validate(true)                                     // 验证元素是否正确-与图Schema是否对应
        .skipInvalidKnowledges(false)                      // 是否跳过不合规的元素-该条数据可另作处理或丢弃
        .hostname("hostname")                            //  (必填)接收数据的主机名
        .port(9999)                                         //  (必填)接收数据的端口
        .delimiter("\n")                                   // 每一行数据的分隔符号,默认\n
        .exec(user);
```

##### 自定义数据源和使用Flink ETL
为了适应高级场景，充分使用到Flink流批处理能力，AbutionGraph除了封装以上常用的消息中间件外，我们为Flink高级开发人员开放了AbutionSink，使您可以应用到Flink的所有功能，自行接入更多种类的数据源，经过复杂的数据处理和转换为图数据后最终流入AbutionGraph。要使用AbutionSink很简单，您只需提供几个参数就可以把源源不断的数据流接入到AbutionGraph中，因为AbutionSink经过优化，数据的实时吞吐量可在几十万～几千万之间，这取决于您的集群资源情况。  
Sink是流的重点，所以AbutionSink接入的输入也必须为单一Knowledge（Entity/Edge），AbutionSink的三个参数：
  1. graphId
  2. schema
  3. properties

如果graph不存在则使用schema和properties新建。  

针对文件流，您还可以在Flink中使用AbutionOutput将数据接入到AbutionGraph中。  

扩展：您还可以使用Kafka或者Flink自身来实现分布式事务，这将变得很轻松。  

**FAQs**
在集群上运行Flink操作时出现错误：使用Flink功能时别忘了启用它，还有一点，别把启用了Flink功能的图配置更新到AbutionGraphLibrary。如要分发Job，Flink要求Job的所有组件都可序列化。



### Spark数据源  
Spark的扩展不包含在原始图功能中，使用前请先在AbutionProperties中启用扩展，如下所示：
```java
AbutionProperties properties = null;
properties.enableGetDataFrameOfKnowledge();
properties.enableGetGraphFrameOfKnowledge();
properties.enableGetRDDOfKnowledge();
properties.enableImportKeyValueJavaPairRDDToAbution();
//.....
```
**注意：**  
除了属性字段外，为了匹配到维度名称、源点和终点、边方向等信息，  
1. Edge DataFrame必须包含这些列：( "dimension", "vertex", "src", "dst", "directed", "matchedVertex" )
2. Entity DataFrame必须具有这些列：( "dimension", "vertex"/"id" )

使用Spark进行入库，请确保DataFrame/GraphFrame中的字段在AbutionGraph的Schema中已经定义过。

#### AddKnowledgeFromDataFrame
将DataFrame转成Knowledges并写入AbutionGDB，此操作需要目标图Schema与写入数据类型一致。  

DataFrame是Edge则必须具有列：  
1. dimension（String）
2. src
3. dst
4. directed（Boolean，是否有向边）
5. matchedVertex（Boolean，可为null）
6. 其它列为属性

DataFrame是Entity则必须具有列：  
1. dimension（String）
2. vertex/id
3. 其它列为属性

```scala
val dataDF:DataFrame = ...

val addKnowledge = new AddKnowledgeFromDataFrame.Builder()
  .input( dataDF )
  .validate(false)
  .skipInvalidKnowledges(true)
  .build
```

#### AddKnowledgeFromGraphFrame
将GraphFrameToIterableRow转成AbutionGDB，此操作需要目标图Schema与写入数据类型一致。  

GraphFrame是Edge则必须具有列：
1. dimension（String）
2. src
3. dst
4. directed（Boolean，是否有向边）
5. matchedVertex（Boolean，可为null）
6. 其它列为属性

GraphFrame是Entity则必须具有列：
1. dimension（String）
2. vertex/id
3. 其它列为属性

```scala
val dataGF:GraphFrame = ...

val addKnowledge = new AddKnowledgeFromGraphFrame.Builder()
  .input( dataGF )
  .validate(false)                               // 验证每个元素是否正确-与图Schema是否一致,不一致的Entity/Edge将不能入库
  .skipInvalidKnowledges(true)                   // 是否跳过验证失败的元素,继续入库其余元素
  .build
```

Ps：如果GraphFrame中不存在任何实体维度的数据，将只会入库Edges的数据。

### Hadoop数据源
#### AddKnowledgeFromHdfs
该操作针对超大型的数据上传任务，我们采用Hadoop的MapReduce框架进行批量上传数据到AbutionGraph，可上传HDFS文件或者本地文件。  
HDFS文件需在文件路径前标识： hdfs://host:port  
服务器本地文件需在文件路径前标识： file:  

```java
String dataPath =    "hdfs://www.thutmose.cn:9000/testJY.csv"; //或者 file:/home/raini/testJY.csv";
// 以下属性为配置MapReduce运行过程中产生的临时数据存放文件,可原封不懂照抄
String outputPath =     "/tmp/outputPath";
String failurePath =    "/tmp/failurePath";
String splitsFilePath = "/tmp/splitsFilePath";
String workingPath =    "/tmp/workingPath";

g.dataSource().hdfs()
        .addInputMapperPair(dataPath, TextMapperGeneratorExample.class) // 可对每个文件指定不同数据处理逻辑程序
        .outputPath(outputPath)
        .failurePath(failurePath)
        .splitsFilePath(splitsFilePath)
        .workingPath(workingPath)
        .jobInitialiser(new TextJobInitialiser())    // 默认作业初始化程序为文本数据任务,Avro格式可选AvroJobInitialiser
        .minReducers(10)                                  //
        .maxReducers(100)                                 // 指定Reducer的最大最小区间-程序会自动选用一个合适的值
        .exec(user);

/** -------------------------(数据处理程序demo)--------------------------------------------- */

public static class TextMapperGeneratorImpl extends TextMapperGenerator {
    public TextMapperGeneratorImpl() { super(new ExampleGenerator() ); }
}
public static class ExampleGenerator implements OneToManyKnowledgeGenerator<String> {
    @Override
    public Iterable<Knowledge> _apply(String record) {
    	List<Entity> entities = new ArrayList<>();
    	List<Edge> edges = new ArrayList<>();
      String[] parts = record.split(",");
    	/**
    	  在此编写对每一条数据的处理逻辑, 将其处理成Entity和Edge对象后 添加到对应List中即可
    	 */
    	//new Entity(parts[0], parts[1]); Entity(dimension, vertex, properties) ; new Edge(,) ...
    	return new ChainedIterable<>(entities, edges);
    }
}
```

**设置分割点（可选）**  
在执行AddKnowledgeFromHdfs之前设置数据分割点有助于提高初始读写速率，这些分割点预先采样将数据拆分为大小近似相等的分区，使程序可以并行进行读写操作，而无需等待自动拆分，默认情况下不手动执行分隔操作我们也会对新建的图执行该操作，并采样1%的数据。

执行方式：
1. addKnowledgeFromHdfs.runSplitPointOperation(graph, user);  // 在该图上使用即将导入的数据生成分隔点
2. g.execute(addKnowledgeFromHdfs, user);                 // 执行数据导入

打包运行命令：
> AddKnowledgeFromHdfs程序需要打包成jar在本地运行：hadoop jar abution-bulkload.jar conf...

HD运行环境依赖（默认已包含）：
> abution-bulkload-ext.jar、abution-store-ext-lib.jar 移动到 /thutmose/app/hadoop/share/hadoop/hdfs/lib/

### Kafka数据源
#### AddKnowledgeConsumeKafka
AbutionGraph与Kafka的直接连接程序，Abution直接消费Kafka中的数据，并写入数据库中。
```java
// kafka消费者详细设置（可选）
java.util.Properties props = new java.util.Properties();
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());   //key反序列化器
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); //value反序列化器
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest"); // earliest latest
props.put(ConsumerConfig.RETRY_BACKOFF_MS_CONFIG, "20000");
props.put(ConsumerConfig.CLIENT_ID_CONFIG, "producer-1");
//max.poll.interval.ms

// AbutionKafkaConnector
g.dataSource().kafka()
        bootstrapServers("60.205.254.97:9092").  //Broker列表
        groupId("abutionGroup").  //消费者组
        topics("abutionTopic").   //
        consumerConfig(props).
        generator(cn.thutmose.abution.test.kafkaload.AbutionFlinkKafkaGeneratorImpl.class). //数据转换函数
        exec(user);
```

### Tuning
#### GraphFlush
AbutionGraph会定时将memstore中的数据合并并溢写到磁盘文件，您也可以手动让数据库执行此步骤。
```java
  g.flush();
```
#### GraphCompact
为了提高查询效率，AbutionGraph会定时将数据小文件合并成大文件，您也可以手动让数据库执行此步骤。
```java
  g.compact();
```
#### AddSplitPoint
在分布式环境下大规模的写入数据，提前设置Graph数据分隔点有助于提高初始性能。  
对于实时数据流，预先使用数据生成分隔点是有效的选择；  
对于离线批量导入数据，这是提高初始性能的最佳选择。  
```java
  g.user().addParallelPoints(Lists.newArrayList("0","4","6","8","a","f","i","l","m"));
```
拆分点数量自行设计，默认为节点数量。单机环境下提高该参数值对性能提高无明显提高。

### AggregatorUtil
对于物联网监控等场景聚合粒度细、时效性要求高的场景，使用Abution聚合可能不是最优化的方法，那么我们可以使用AggregatorUtil对写入的数据提前聚合一遍，如在内存、Flink或者Kafka的时间窗口中聚合每分钟内的数据，我们可以在时间窗口内做预警，这也可以减少写入量后，再将分钟粒度的数据写入Abution。
```java
  List<Knowledge> elementList = null;

  Iterable<? extends Knowledge> aggKnowledges = AggregatorUtil.ingestAggregate(elementList,schema);
```
